{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahat-ansari/AI-ML-CS50-Lecture-0-Search-Algorithm-Maze-Solver/blob/main/how_to_count_the_objects_using_ultralytics_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/how-to-count-the-objects-using-ultralytics-yolo.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  \n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "  \n",
        "  Welcome to the Object counting using Ultralytics YOLO11 üöÄ notebook! <a href=\"https://github.com/ultralytics/ultralytics\">YOLO11</a> is the latest version of the YOLO (You Only Look Once) AI models developed by <a href=\"https://ultralytics.com\">Ultralytics</a>. We hope that the resources in this notebook will help you get the most out of YOLO11. Please browse the YOLO11 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Counting using Ultralytics YOLO11\n",
        "\n",
        "This notebook serves as a starting point for [counting objects](https://docs.ultralytics.com/guides/object-counting/) in videos or live streams using the YOLO11 model.\n",
        "\n",
        "### What is Object Counting?\n",
        "\n",
        "- Object counting with YOLO11 involves accurate identification and counting of specific objects in videos and camera streams. YOLO11 excels in real-time applications, providing efficient and precise object counting for various scenarios like crowd analysis and surveillance, thanks to its state-of-the-art algorithms and deep learning capabilities.\n",
        "\n",
        "### Advantages of Object Counting?\n",
        "\n",
        "- **Resource Optimization**: Object counting facilitates efficient resource management by providing accurate counts, and optimizing resource allocation in applications like inventory management.\n",
        "- **Enhanced Security**: Object counting enhances security and surveillance by accurately tracking and counting entities, aiding in proactive threat detection.\n",
        "- **Informed Decision-Making**: Object counting offers valuable insights for decision-making, optimizing processes in retail, traffic management, and various other domains."
      ],
      "metadata": {
        "id": "7EM2nwU4jshF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "### Setup\n",
        "\n",
        "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55211420-8206-47dd-c2fa-61aef8cb4796"
      },
      "source": [
        "!uv pip install ultralytics\n",
        "\n",
        "import ultralytics\n",
        "import cv2\n",
        "from ultralytics.utils.downloads import safe_download\n",
        "from ultralytics import solutions\n",
        "\n",
        "ultralytics.checks()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.198 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 39.0/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the Video File\n",
        "\n",
        "- You can either read the video file directly or stream the content from an RTSP (Real-Time Streaming Protocol) source, allowing for flexible video input depending on your needs.\n",
        "- We will also set up the video writer to handle the output video writing."
      ],
      "metadata": {
        "id": "h8go3HNgN0WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safe_download(\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/solutions-ci-demo.mp4\")\n",
        "cap = cv2.VideoCapture(\"solutions-ci-demo.mp4\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,\n",
        "                                       cv2.CAP_PROP_FRAME_HEIGHT,\n",
        "                                       cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Video writer\n",
        "video_writer = cv2.VideoWriter(\"counting.avi\",\n",
        "                               cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                               fps, (w, h))"
      ],
      "metadata": {
        "id": "QUgMYUvlNLvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Region Coordinates\n",
        "\n",
        "Here, we set the coordinates for specific regions to ensure accurate object tracking and analysis within the video or stream. This helps monitor and count objects effectively in different areas."
      ],
      "metadata": {
        "id": "3wJlBXORXNsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define region points\n",
        "# region_points = [(20, 400), (1080, 400)]  # For line counting\n",
        "region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # For rectangle region counting\n",
        "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]  # For polygon region counting"
      ],
      "metadata": {
        "id": "bVCrrForXRgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the ObjectCounter Class\n",
        "\n",
        "- Now, let's initialize the `ObjectCounter` class to track and count objects in each frame of the video."
      ],
      "metadata": {
        "id": "rt3soEHzXe8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Init ObjectCounter\n",
        "counter = solutions.ObjectCounter(\n",
        "    show=True,  # Display the output\n",
        "    region=region_points,  # Pass region points\n",
        "    model=\"yolo11n.pt\",  # model=\"yolo11n-obb.pt\" for object counting using YOLO11 OBB model.\n",
        "    # classes=[0, 2],  # If you want to count specific classes i.e person and car with COCO pretrained model.\n",
        "    # show_in=True,  # Display in counts\n",
        "    # show_out=True,  # Display out counts\n",
        "    # line_width=2,  # Adjust the line width for bounding boxes and text display\n",
        ")"
      ],
      "metadata": {
        "id": "Va24DpUZXTh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2575928-a239-4a44-c850-4eb86e053152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics Solutions: ‚úÖ {'source': None, 'model': 'yolo11n.pt', 'classes': None, 'show_conf': True, 'show_labels': True, 'region': [(20, 400), (1080, 400), (1080, 360), (20, 360)], 'colormap': 21, 'show_in': True, 'show_out': True, 'up_angle': 145.0, 'down_angle': 90, 'kpts': [6, 8, 10], 'analytics_type': 'line', 'figsize': (12.8, 7.2), 'blur_ratio': 0.5, 'vision_point': (20, 20), 'crop_dir': 'cropped-detections', 'json_file': None, 'line_width': 2, 'records': 5, 'fps': 30.0, 'max_hist': 5, 'meter_per_pixel': 0.05, 'max_speed': 120, 'show': True, 'iou': 0.7, 'conf': 0.25, 'device': None, 'max_det': 300, 'half': False, 'tracker': 'botsort.yaml', 'verbose': True, 'data': 'images'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Video Frames\n",
        "\n",
        "In this step, we will process each frame of the video to detect and analyze objects. This allows for real-time tracking and counting, based on the visual data in the frames."
      ],
      "metadata": {
        "id": "1ewYRFFqXvtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process video\n",
        "while cap.isOpened():\n",
        "    success, im0 = cap.read()\n",
        "    if not success:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "    results = counter(im0)  # count the objects\n",
        "    video_writer.write(results.plot_im)   # write the video frames\n",
        "\n",
        "cap.release()   # Release the capture\n",
        "video_writer.release()"
      ],
      "metadata": {
        "id": "PVf1pyRtXijz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c8e85d-09dd-4719-9542-69a4da737ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 360x640 2.3ms, 17 person\n",
            "Speed: 806.8ms track, 2.3ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "1: 360x640 3.2ms, 17 person\n",
            "Speed: 29.0ms track, 3.2ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "2: 360x640 2.4ms, 14 person\n",
            "Speed: 29.4ms track, 2.4ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "3: 360x640 2.5ms, 14 person\n",
            "Speed: 26.9ms track, 2.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "4: 360x640 2.6ms, 15 person\n",
            "Speed: 25.8ms track, 2.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "5: 360x640 2.8ms, 16 person\n",
            "Speed: 27.3ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "6: 360x640 2.7ms, 16 person\n",
            "Speed: 26.9ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "7: 360x640 3.5ms, 16 person\n",
            "Speed: 25.4ms track, 3.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "8: 360x640 4.0ms, 16 person\n",
            "Speed: 27.2ms track, 4.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "9: 360x640 2.9ms, 17 person\n",
            "Speed: 37.0ms track, 2.9ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "10: 360x640 2.7ms, 16 person\n",
            "Speed: 26.9ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "11: 360x640 2.8ms, 16 person\n",
            "Speed: 26.2ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "12: 360x640 2.8ms, 16 person\n",
            "Speed: 26.5ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "13: 360x640 2.7ms, 16 person\n",
            "Speed: 24.7ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "14: 360x640 2.8ms, 16 person\n",
            "Speed: 26.5ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "15: 360x640 2.8ms, 16 person\n",
            "Speed: 28.7ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "16: 360x640 2.8ms, 16 person\n",
            "Speed: 28.0ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "17: 360x640 2.7ms, 15 person\n",
            "Speed: 27.9ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "18: 360x640 2.7ms, 16 person\n",
            "Speed: 24.8ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "19: 360x640 3.0ms, 16 person\n",
            "Speed: 23.7ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "20: 360x640 3.1ms, 16 person\n",
            "Speed: 43.9ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "21: 360x640 2.5ms, 15 person\n",
            "Speed: 27.3ms track, 2.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "22: 360x640 2.5ms, 15 person\n",
            "Speed: 27.1ms track, 2.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "23: 360x640 2.8ms, 15 person\n",
            "Speed: 28.2ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "24: 360x640 2.7ms, 15 person\n",
            "Speed: 27.1ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "25: 360x640 2.6ms, 15 person\n",
            "Speed: 27.5ms track, 2.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "26: 360x640 2.6ms, 14 person\n",
            "Speed: 29.4ms track, 2.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "27: 360x640 3.0ms, 17 person\n",
            "Speed: 28.9ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "28: 360x640 3.0ms, 17 person\n",
            "Speed: 28.2ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "29: 360x640 3.0ms, 17 person\n",
            "Speed: 27.8ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "30: 360x640 2.9ms, 17 person\n",
            "Speed: 30.5ms track, 2.9ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "31: 360x640 2.9ms, 17 person\n",
            "Speed: 26.6ms track, 2.9ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "32: 360x640 3.1ms, 17 person\n",
            "Speed: 28.1ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "33: 360x640 2.8ms, 16 person\n",
            "Speed: 29.8ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "34: 360x640 3.1ms, 18 person\n",
            "Speed: 28.7ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "35: 360x640 3.0ms, 17 person\n",
            "Speed: 28.0ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "36: 360x640 3.1ms, 17 person\n",
            "Speed: 28.4ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "37: 360x640 3.3ms, 17 person\n",
            "Speed: 26.2ms track, 3.3ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "38: 360x640 3.0ms, 17 person\n",
            "Speed: 26.6ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "39: 360x640 3.2ms, 17 person\n",
            "Speed: 27.9ms track, 3.2ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "40: 360x640 3.0ms, 16 person\n",
            "Speed: 26.8ms track, 3.0ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "41: 360x640 2.8ms, 16 person\n",
            "Speed: 31.4ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "42: 360x640 2.8ms, 16 person\n",
            "Speed: 27.4ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "43: 360x640 3.2ms, 16 person\n",
            "Speed: 25.6ms track, 3.2ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "44: 360x640 2.8ms, 16 person\n",
            "Speed: 28.8ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "45: 360x640 2.7ms, 15 person\n",
            "Speed: 29.4ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "46: 360x640 2.7ms, 16 person\n",
            "Speed: 27.3ms track, 2.7ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "47: 360x640 3.1ms, 17 person\n",
            "Speed: 26.0ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "48: 360x640 2.8ms, 15 person\n",
            "Speed: 26.7ms track, 2.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "49: 360x640 5.1ms, 15 person\n",
            "Speed: 28.3ms track, 5.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "50: 360x640 2.6ms, 15 person\n",
            "Speed: 35.4ms track, 2.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "51: 360x640 2.9ms, 17 person\n",
            "Speed: 29.7ms track, 2.9ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "52: 360x640 3.1ms, 18 person\n",
            "Speed: 32.5ms track, 3.1ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "53: 360x640 3.8ms, 21 person\n",
            "Speed: 30.8ms track, 3.8ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "54: 360x640 3.6ms, 21 person\n",
            "Speed: 32.3ms track, 3.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "55: 360x640 3.5ms, 21 person\n",
            "Speed: 28.0ms track, 3.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "56: 360x640 5.4ms, 19 person\n",
            "Speed: 30.7ms track, 5.4ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "57: 360x640 3.2ms, 19 person\n",
            "Speed: 33.8ms track, 3.2ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "58: 360x640 3.5ms, 21 person\n",
            "Speed: 29.3ms track, 3.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "59: 360x640 3.5ms, 21 person\n",
            "Speed: 28.3ms track, 3.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "60: 360x640 3.5ms, 21 person\n",
            "Speed: 28.7ms track, 3.5ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "61: 360x640 3.6ms, 21 person\n",
            "Speed: 27.7ms track, 3.6ms solution per image at shape (1, 3, 360, 640)\n",
            "\n",
            "Video frame is empty or video processing has been successfully completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fish Counting in the Sea Using Ultralytics YOLO11](https://github.com/ultralytics/docs/releases/download/0/conveyor-belt-packets-counting.avif)"
      ],
      "metadata": {
        "id": "bWskbLSKH2S5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwBUa5kZyZ2k"
      },
      "source": [
        "Crafted with üíô by [Ultralytics](https://ultralytics.com/)  \n",
        "\n",
        "üåü Explore and star the [Ultralytics Notebooks](https://github.com/ultralytics/notebooks/) to supercharge your AI journey! üöÄ"
      ]
    }
  ]
}